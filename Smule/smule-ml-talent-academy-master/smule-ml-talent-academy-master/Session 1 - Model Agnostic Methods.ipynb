{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Agnostic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As models become more and more complex, our ability to understand them with our monkey brains decreases. We can understand how they function, but it becomes impossible to follow the exact mathematical inner-workings. This is what is widely known as a **black-box** model. There is a widely-shared notion that black box models are uninterpretable and we cannot understand how they work.\n",
    "\n",
    "In my opinion, this is lazy and is spread by fearmongers who have never attempted it. As a whole, models are deterministic. Even if we can't see what is going on inside, we can still observe the input and the output. We can fall back to the naive, childish ways of Deedee \"Ooh, what does this button do?\". \n",
    "\n",
    "This is the essence of model-agnostic methods. We modify the input in a controlled way and observe the change in output in order to reason about the behaviour of the model. \n",
    "\n",
    "This notebook will explore the following techniques:\n",
    "\n",
    "1. Partial Dependence Plot (PDP) - observe the mean prediction as a given feature is set to a given level for the **entire** dataset\n",
    "2. Permutation Feature Importance - measure the dip in performance as information about a given feature is lost (via permutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import fastprogress\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('talk')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIME</th>\n",
       "      <th>T2M_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-5.84125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-3.54462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.18427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.30780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.10742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TIME     T2M_\n",
       "0     0 -5.84125\n",
       "1     0 -3.54462\n",
       "2     1 -2.18427\n",
       "3     1 -0.30780\n",
       "4     2  2.10742"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/Nasa.csv\", usecols = [1,4])\n",
    "# data['brand'] = data['name'].apply(lambda x: x.split(' ')[0])\n",
    "# data['selling_price'] = data['selling_price'] * 0.014\n",
    "# data['selling_price'] = data['selling_price'] / 1000\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = pd.read_csv(\"data/valid.csv\",index_col=0)\n",
    "valid_ids = valid_ds.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Neural Network via fast.ai\n",
    "\n",
    "The fast.ai library provides us with a modern neural net architecture for tabular data that can handle categorical variables by associating them with learned embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai\n",
      "  Using cached fastai-2.0.12-py3-none-any.whl (355 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from fastai) (20.4)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastai) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch>=1.6.0 (from fastai) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\n",
      "ERROR: No matching distribution found for torch>=1.6.0 (from fastai)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-06519ce54d22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install fastai'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtabular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastai'"
     ]
    }
   ],
   "source": [
    "!pip install fastai\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = [\"year\", \"fuel\", \"seller_type\", \"transmission\", \"owner\", \"brand\"]\n",
    "\n",
    "dls = TabularDataLoaders.from_df(data, \n",
    "                                 cat_names=CATEGORICAL_FEATURES, \n",
    "                                 cont_names=[\"km_driven\"],\n",
    "                                 valid_idx=valid_ids,\n",
    "                                 y_names=\"selling_price\", \n",
    "                                 procs=[Categorify, FillMissing, Normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls,loss=mse, metrics=rmse)\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(12, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(12, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(12, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(12, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Plot\n",
    "\n",
    "The procedure to obtain a partial dependence plot is as follows: for a given feature, define a set of feature values. For each feature value, copy the input data but replace the original feature value with the surrogate. Afterwards, run inference and record the mean prediction. The PDP is simply a plot of the mean predictions (over the entire dataset) at each feature value.\n",
    "\n",
    "For example, for continuous variables, we might decide to evaluate the PDP at every (max - min) / 100 feature value.\n",
    "For categorical variables, it is even easier - simply observe the mean prediction when we substitute the feature values for the entire dataset with each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = \"km_driven\"\n",
    "min_fv = data[feat].min()\n",
    "max_fv = data[feat].max()\n",
    "preds_by_fv = {}\n",
    "for fv in fastprogress.progress_bar(np.arange(min_fv, max_fv, (max_fv - min_fv)/ 100)):\n",
    "    modified_df = data.copy()\n",
    "    modified_df[feat] = fv\n",
    "    modified_dl = learn.dls.test_dl(modified_df)\n",
    "    with learn.no_bar():\n",
    "        preds = learn.get_preds(dl=modified_dl)\n",
    "    preds_by_fv[fv] = preds[0].mean().item()\n",
    "ax = pd.Series(preds_by_fv).plot(figsize=(16,9),lw=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(cat_feat=CATEGORICAL_FEATURES)\n",
    "def plot_cat_feat_pdp(cat_feat=\"year\"):\n",
    "    preds_by_fv = {}\n",
    "    for fv in learn.dls.categorify[cat_feat]:\n",
    "        if fv == '#na#': continue\n",
    "        modified_df = data.copy()\n",
    "        modified_df[cat_feat] = fv\n",
    "        modified_dl = learn.dls.test_dl(modified_df)\n",
    "        with learn.no_bar():  preds = learn.get_preds(dl=modified_dl)\n",
    "        preds_by_fv[fv] = preds[0].mean().item()\n",
    "    ax = pd.Series(preds_by_fv).sort_values().plot(kind='barh',figsize=(16,9))\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_title(f\"Partial Dependence Plot for {feat}\")\n",
    "    ax.set_xlabel(\"Mean Prediction\")\n",
    "    ax.set_ylabel(\"Feature Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Feature Importance\n",
    "\n",
    "Permutation feature importance attemps to quantify the utility of a given feature by breaking the connection between the feature and the target variable.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "1. Pick a feature\n",
    "2. Randomly shuffle the feature for the validation dataset\n",
    "3. Measure the relative drop in prediction quality\n",
    "\n",
    "The reasoning is that permuting the important features will result in a higher drop because the model relies on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_before = rmse(*predict_with_ds(valid_ds))\n",
    "error_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_ds(ds):\n",
    "    dl = learn.dls.test_dl(ds)\n",
    "    with learn.no_bar():\n",
    "        return learn.get_preds(dl=dl)\n",
    "\n",
    "def rmse(preds, targets):\n",
    "    return math.sqrt(((preds - targets)**2).mean().item())\n",
    "\n",
    "\n",
    "def calculate_permutation_loss(feat):\n",
    "    permuted = valid_ds.copy()\n",
    "    permuted[feat] = np.random.permutation(permuted[feat])\n",
    "    error_after = rmse(*predict_with_ds(permuted))\n",
    "    return error_after - error_before\n",
    "    \n",
    "calculate_permutation_loss(\"km_driven\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since randomness is involved in the process, it helps to repeat the process a number of times and take the mean. \n",
    "\n",
    "Let's calculate the importance for each feature and analyze the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_losses = {}\n",
    "for feat in [\"km_driven\"] + CATEGORICAL_FEATURES:\n",
    "    # repeat the experiment 10 times and take the mean\n",
    "    feature_losses = np.mean([calculate_permutation_loss(feat) for _ in range(10)])\n",
    "    permutation_losses[feat] = feature_losses\n",
    "ax = pd.Series(permutation_losses).sort_values().plot(figsize=(16,9), kind='barh')\n",
    "sns.despine(ax=ax)\n",
    "ax.set_title(\"Permutation Feature Importance\")\n",
    "ax.set_xlabel(\"Increase in RMSE\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
